[model]
# encoder parameters
encoder = 'bert'
bert = 'bigscience/bloom-1b7'
n_bert_layers = 0
mix_dropout = .0
bert_pooling = 'mean'
encoder_dropout = .1
n_encoder_hidden = 100

# decoder parameters
n_decoder_layers = 2
mlp_dropout = .33

[optim]
# learning rate parameters
lr = 5e-5
lr_rate = 3
warmup = 0.001
update_steps = 1

# optimizer parameters
mu = 0.9
nu = 0.999
eps = 1e-8
weight_decay = 0

# training parameters
epochs = 100
batch_size = 1000
patience = 20


[vq]
codebook_size = 512
vq_decay = 0.8
commitment_weight = 2
vq_passes = 300