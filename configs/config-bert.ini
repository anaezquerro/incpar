[model]
# encoder parameters
encoder = 'bert'
bert = 'bert-base-uncased'
n_bert_layers = 0
mix_dropout = .0
bert_pooling = 'mean'
encoder_dropout = .1
n_encoder_hidden = 100

# decoder parameters
mlp_dropout = .33
n_decoder_layers = 2

[optim]
# learning rate parameters
lr = 5e-5
lr_rate = 10
warmup = 0.001
update_steps = 1

# optimizer parameters
clip = 5.0
min_freq = 2
fix_len = 20
mu = 0.9
nu = 0.999
eps = 1e-8
weight_decay = 0

# training parameters
epochs = 100
patience = 10
batch_size = 2000

