[model]
# encoder parameters
encoder = 'bert'
bert = 'xlm-roberta-large'
n_bert_layers = 0

[optim]
# learning rate parameters
lr = 5e-5
lr_rate = 10
warmup = 0.001
update_steps = 10

# optimizer parameters
clip = 5.0
min_freq = 2
fix_len = 20
mu = 0.9
nu = 0.999
eps = 1e-12
weight_decay = 0

# training parameters
epochs = 100
patience = 10
batch_size = 1000

