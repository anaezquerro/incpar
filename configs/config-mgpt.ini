[model]
# encoder parameters
encoder = 'bert'
bert = 'sberbank-ai/mGPT'
n_bert_layers = 0
n_plm_embed = 100
mix_dropout = .0
bert_pooling = 'mean'
encoder_dropout = .1
n_encoder_hidden = 100


# decoder parameters
mlp_dropout = .33
n_decoder_layers = 2

[optim]
# learning rate parameters
lr = 5e-5
lr_rate = 10
warmup = 0.001
update_steps = 1

# optimizer parameters
mu = 0.9
nu = 0.999
eps = 1e-8
weight_decay = 0

# training parameters
epochs = 30
patience = 10
batch_size = 1500

[vq]
codebook_size = 512
vq_decay = 0.3
commitment_weight = 0.4
vq_passes = 600