[model]
# embedding parameters
feat = ['tag', 'char']
n_embed = 300
n_pretrained = 300
n_char_embed = 50
n_char_hidden = 100
n_feat_embed = 100
embed_dropout = .33

# encoder parameters
encoder = 'lstm'
bidirectional = True
n_encoder_layers = 4
n_lstm_hidden = 400
encoder_dropout = .33
n_encoder_hidden = 400

# decoder parameters
n_decoder_layers = 2
mlp_dropout = .33

[optim]
# learning rate parameters
lr = 1e-3
decay = 0.1
decay_steps = 5000
update_steps = 1

# optimizer parameters
mu = .9
nu = .9
eps = 1e-12
weight_decay = 0

# training parameters
epochs = 200
patience = 20
batch_size = 5000

[vq]
codebook_size = 512
vq_decay = 0.3
commitment_weight = 0.4
vq_passes = 600
